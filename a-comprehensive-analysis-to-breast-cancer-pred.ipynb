{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Breast cancer predicting model - ML\n\nBased on this dataset: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n\n\n> Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\n> This database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n> Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"},{"metadata":{},"cell_type":"markdown","source":"# 0 - Intro\n\n\n\n\n**My main objective is to build 5 models of machine learning and 1 model of deep learning to predict if the tumor is a benign one or not.\nBesides, I am going to do the tuning of parameters of each model to get a better accuracy.\nThen, I perform the comparison of accuracy between these models.**\n\n\n**Table of contents**\n\n* [1 - Exploratory data analysis](#1-Exploratory-Data-Analysis)\n* [2 - Dataset preparation](#2-Dataset-Preparation)\n* [3 - Create train and test datasets](#3-Create-Train-And-Test-Datasets)\n* [4 - Models](#4-Models)\n    - [4.1 - SVM](#4.1-SVM)\n        - [4.1.1 - SVM - Build a model with default parameters](#4.1.1-Build-A-Model-With-Default-Parameters)\n        - [4.1.2 - SVM - Parameters tuning](#4.1.2-Parameters-Tuning)\n        - [4.1.3 - SVM - Confusion Matrix](#4.1.3-Confusion-Matrix)\n        - [4.1.4 - SVM - Importance of each feature](#4.1.4-Importance-Of-Each-Feature)\n        - [4.1.5 - SVM - Cross Validation](#4.1.5-Cross-Validation)\n    - [4.2 - Decision Tree](#4.2-Decision-Tree)\n        - [4.2.1 - Decision Tree - Build a model with default parameters](#4.2.1-Build-A-Model-With-Default-Parameters)\n        - [4.2.2 - Decision Tree - Parameters tuning](#4.2.2-Parameters-Tuning)\n        - [4.2.3 - Decision Tree - Confusion Matrix](#4.2.3-Confusion-Matrix)\n        - [4.2.4 - Decision Tree - Importance of each feature](#4.2.4-Importance-Of-Each-Feature)\n        - [4.2.5 - Decision Tree - Cross Validation](#4.2.5-Cross-Validation)\n    - [4.3 - Logistic regression](#4.3-Logistic-Regression)\n        - [4.3.1 - Logistic Regression - Build a model with default parameters](#4.3.1-Build-A-Model-With-Default-Parameters)\n        - [4.3.2 - Logistic Regression - Parameters tuning](#4.3.2-Parameters-Tuning)\n        - [4.3.3 - Logistic Regression - Confusion Matrix](#4.3.3-Confusion-Matrix)\n        - [4.3.4 - Logistic Regression - Importance of each feature](#4.3.4-Importance-Of-Each-Feature)\n        - [4.3.5 - Ligistic Regression - Cross Validation](#4.3.5-Cross-Validation)\n    - [4.4 - Random forest](#4.4-Random-Forest)\n        - [4.4.1 - Random Forest - Build a model with default parameters](#4.4.1-Build-A-Model-With-Default-Parameters)\n        - [4.4.2 - Random Forest - Parameters tuning](#4.4.2-Parameters-Tuning)\n        - [4.4.3 - Random Forest - Confusion Matrix](#4.4.3-Confusion-Matrix)\n        - [4.4.4 - Random Forest - Importance of each feature](#4.4.4-Importance-Of-Each-Feature)\n        - [4.4.5 - Random Forest - Cross Validation](#4.4.5-Cross-Validation)\n    - [4.5 - KNN](#4.5-KNN)\n        - [4.5.1 - KNN - Build a model with default parameters](#4.5.1-Build-A-Model-With-Default-Parameters)\n        - [4.5.2 - KNN - Parameters tuning](#4.5.2-Parameters-Tuning)\n        - [4.5.3 - KNN - Confusion Matrix](#4.5.3-Confusion-Matrix)\n        - [4.5.4 - KNN - Importance of each feature](#4.5.4-Importance-Of-Each-Feature)\n        - [4.5.5 - KNN - Cross Validation](#4.5.5-Cross-Validation)\n* [5 - Deep learning - Tensorflow and Keras](#5-Deep-Learning-TensorFlow-And-Keras)\n    - [5.1 - Using test_split](#5.1-Using-Test-Split)\n    - [5.2 - Using cross validation](#5.2-Using-Cross-Validation)\n* [6 - Conclusion](#6-Conclusion)\n* [7 - Appendix](#7-Appendix)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1-Exploratory-Data-Analysis\"></a>\n# 1 - Exploratory data analysis\n\n **Attribute Information:**\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign) \n\n3)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 / area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant\n\nhttps://www.kaggle.com/uciml/breast-cancer-wisconsin-data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn import neighbors\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import r2_score,confusion_matrix, accuracy_score, plot_confusion_matrix #utilizada para verificar a acurácia do modelo construído\nfrom sklearn.svm import SVC #utilizada para importar o algoritmo SVM\n#from sklearn.linear_model import LinearRegression\n#import tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Necessary functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def DisplayConfusionMatrix(_classifier, _title):\n    df_cm_svm = pd.DataFrame(_classifier, index = [i for i in \"01\"],columns = [i for i in \"01\"])\n    disp = plot_confusion_matrix(_classifier, x_test, y_test,\n                                 display_labels = ['Benign','Malignant'],\n                                 cmap = plt.cm.Blues,\n                                 normalize = None)\n    disp.ax_.set_title(_title)\n\n    #print(_title)\n    #print(disp.confusion_matrix)\n\n#https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56\n\ndef DisplayConfusionMatrix_2(_y_pred, _title):\n    matrix = confusion_matrix(y_test, _y_pred)\n    #matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]  # if we want %\n\n    # Build the plot\n    plt.figure(figsize=(7,7))\n    sns.set(font_scale=1.4)\n    sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=plt.cm.Blues, linewidths=0.2)\n\n    # Add labels to the plot\n    class_names = ['Benign','Malignant']\n    tick_marks = np.arange(len(class_names))\n    tick_marks2 = tick_marks + 0.5\n    plt.xticks(tick_marks, class_names, rotation=25)\n    plt.yticks(tick_marks2, class_names, rotation=0)\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.title(_title)\n    plt.show()\n    \ndef FeatureImportance (_model):\n    fi = pd.DataFrame({'feature': list(x_train.columns),\n                       'importance': _model.feature_importances_}).sort_values('importance', ascending = False)\n    return fi","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Gets de dataset as a DataFrame\nds = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains 33 columns and 569 records.\nEach record corresponds to  "},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can consider that the data of the column \"diagnosis\" is balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=ds, x='diagnosis')\nplt.title('Count by diagnosis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2-Dataset-Preparation\"></a>\n# 2 - Dataset preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the class column (diagnosis) into a int64\n# 0 = Benign\n# 1 = Malignant\n\n#ds.diagnosis = ds.diagnosis == 'M'\n#ds.diagnosis = ds.diagnosis.astype('int')\n# OR\nds['diagnosis'] = ds['diagnosis'].map({'M':1,'B':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns \"id\" and \"Unnamed: 32\"\nds.drop(['id','Unnamed: 32'], axis= 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3-Create-Train-And-Test-Datasets\"></a>\n# 3 - Create train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create different arrays for features and target\nx = ds.iloc[:, 1:-1]\ny = ds.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation between all features\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n\n#As we can see, there are many features that are strongly correlated. I decided to use all features.\n#To be done: eliminate unuseful features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's split the dataset:\n\n    1. 80% for training the model\n    2. 20% for testing the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train and test datasets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4-Models\"></a>\n# 4 - Models\n\n<a id=\"4.1-SVM\"></a>\n# 4.1 - SVM\n\n<a id=\"4.1.1-Build-A-Model-With-Default-Parameters\"></a>\n# 4.1.1 - SVM - Build a model with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelSVM = SVC(kernel = 'linear')\nmodelSVM.fit(x_train, y_train)\nlabels_svm = modelSVM.predict(x_test)\nscore_svm = modelSVM.score(x_test, y_test)\nprint(\"Score (SVM): %f\" % score_svm)\nconf_mx_svm = confusion_matrix(y_test, labels_svm)\nscores = [score_svm]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.1.2-Parameters-Tuning\"></a>\n# 4.1.2 - SVM - Parameters tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_svm = {'kernel' : ['linear', 'rbf'],   #not used: poly, precomputed and sigmoid\n              'gamma' : ['scale', 'auto']}\ngrid_search_svm = GridSearchCV(estimator = modelSVM,\n                           param_grid = params_svm,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_svm = grid_search_svm.fit(x, y)\nprint(f'The best parameters for SVM are: \"{grid_search_svm.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_svm.best_score_ * 100,2))} %')\nscores.append(grid_search_svm.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.1.3-Confusion-Matrix\"></a>\n# 4.1.3 - SVM - Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"DisplayConfusionMatrix(modelSVM,\"Confusion matrix for SVM model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.1.4-Importance-Of-Each-Feature\"></a>\n# 4.1.4 - SVM - Importance of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To be implemented\n\n#FeatureImportance(modelSVM)\n#def f_importances(coef, names):\n#    imp = coef\n#    imp,names = zip(*sorted(zip(imp,names)))\n#    plt.barh(range(len(names)), imp, align='center')\n#    plt.yticks(range(len(names)), names)\n#    plt.show()\n\n#features_names\n#f_importances(modelSVM.coef_, features_names)\n#modelSVM.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.1.5-Cross-Validation\"></a>\n# 4.1.5 - SVM - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation for SVM with default hyper parameters\nsvm_cv_default = cross_val_score(estimator = modelSVM,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_svm_default_cv = svm_cv_default.mean()\nprint(\"Score (SVM default CV): %f\" % score_svm_default_cv)\nscores_cv = [score_svm_default_cv]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2-Decision-Tree\"></a>\n# 4.2 - Decision Tree\n\n<a id=\"4.2.1-Build-A-Model-With-Default-Parameters\"></a>\n# 4.2.1 - Decision Tree - Build a model with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\nlabels_tree = tree.predict(x_test)\nscore_tree = tree.score(x_test, y_test)\nprint(\"Score (Decision tree): %f\" % score_tree)\nconf_mx_tree = confusion_matrix(y_test, labels_tree)\n#accuracia = accuracy_score(y_test, labels_tree)\n#print (\"Acuracia utilizando o SVM :\" , accuracia , \"\\nEm porcentagem : \", round(accuracia*100) , \"%\\n\")\nscores.append(score_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2.2-Parameters-Tuning\"></a>\n# 4.2.2 - Decision Tree - Parameters tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_tree = {'criterion' : ['gini', 'entropy'],\n               'max_depth' : range(1,10)}\ngrid_search_tree = GridSearchCV(estimator = tree,\n                           param_grid = params_tree,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_tree = grid_search_tree.fit(x, y)\nprint(f'The best parameters for Decision Tree are: \"{grid_search_tree.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_tree.best_score_ * 100,2))} %')\nscores.append(grid_search_tree.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2.3-Confusion-Matrix\"></a>\n# 4.2.3 - Decision Tree - Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"DisplayConfusionMatrix(tree,\"Confusion matrix for Decision Tree model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2.4-Importance-Of-Each-Feature\"></a>\n# 4.2.4 - Decision Tree - Importance of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureImportance(tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2.5-Cross-Validation\"></a>\n# 4.2.5 - Decision Tree - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation for Decision Tree with default hyper parameters\ntree_cv_default = cross_val_score(estimator = tree,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_tree_default_cv = tree_cv_default.mean()\nprint(\"Score (Decision Tree default CV): %f\" % score_tree_default_cv)\nscores_cv.append(score_tree_default_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.3-Logistic-Regression\"></a>\n# 4.3 - Logistic regression\n\n<a id=\"4.3.1-Build-A-Model-With-Default-Parameters\"></a>\n# 4.3.1 - Logistic Regression - Build a model with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(max_iter=3000)\nlogreg.fit(x_train, y_train)\nlabels_logreg = logreg.predict(x_test)\nconf_mx_logreg = confusion_matrix(y_test, labels_logreg)\nscore_lr = logreg.score(x_test, y_test)\nprint(\"Score (Logistic Regression): %f\" % score_lr)\nscores.append(score_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.3.2-Parameters-Tuning\"></a>\n# 4.3.2 - Logistic Regression - Parameters tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_logreg = {\"solver\":[ 'newton-cg', 'liblinear', 'sag', 'saga'],  #not used: 'lbfgs'\n                 \"max_iter\" : [10000]}\ngrid_search_logreg = GridSearchCV(estimator = logreg,\n                           param_grid = params_logreg,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_logreg = grid_search_logreg.fit(x, y)\nscores.append(grid_search_logreg.best_score_)\nprint(f'The best parameters for Logistic Regression are: \"{grid_search_logreg.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_logreg.best_score_ * 100,2))} %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.3.3-Confusion-Matrix\"></a>\n# 4.3.3 - Logistic Regression - Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"DisplayConfusionMatrix(logreg,\"Confusion matrix for Logistic Regression model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.3.4-Importance-Of-Each-Feature\"></a>\n# 4.3.4 - Logistic Regression - Importance of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To be implemented","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.3.5-Cross-Validation\"></a>\n# 4.3.5 - Logistic Regression - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation for Logistic Regression with default hyper parameters\nlogreg_cv_default = cross_val_score(estimator = logreg,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_logreg_default_cv = logreg_cv_default.mean()\nprint(\"Score (Logistic Regression default CV): %f\" % score_logreg_default_cv)\nscores_cv.append(score_logreg_default_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.4-Random-Forest\"></a>\n# 4.4 - Random forest\n\n<a id=\"4.4.1-Build-A-Model-With-Default-Parameters\"></a>\n# 4.4.1 - Random Forest - Build a model with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier()\nforest.fit(x_train, y_train)\nlabels_rf = forest.predict(x_test)\nconf_mx_rf = confusion_matrix(y_test, labels_rf)\nscore_rf = forest.score(x_test, y_test)\nprint(\"Score (Random forest): %f\" % score_rf)\nscores.append(score_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.4.2-Parameters-Tuning\"></a>\n# 4.4.2 - Random Forest - Parameters tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_rf = { \n    'n_estimators': [100, 200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n  #  'max_depth' : ['none', 4,5,6,7,8],\n    'criterion' : ['gini', 'entropy']\n}\n\n#params_rf = {\"criterion\":['gini'], \"n_estimators\" : range(60,110)}\n\ngrid_search_rf = GridSearchCV(estimator = forest,\n                           param_grid = params_rf,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_rf = grid_search_rf.fit(x, y)\nscores.append(grid_search_rf.best_score_)\nprint(f'The best parameters for Random Forest are: \"{grid_search_rf.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_rf.best_score_ * 100,2))} %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.4.3-Confusion-Matrix\"></a>\n# 4.4.3 - Random Forest - Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"DisplayConfusionMatrix_2(labels_rf, 'Confusion Matrix for Random Forest Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.4.4-Importance-Of-Each-Feature\"></a>\n# 4.4.4 - Random Forest - Importance of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureImportance(forest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.4.5-Cross-Validation\"></a>\n# 4.4.5 - Random Forest - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation for Random Fores with default hyper parameters\nrf_cv_default = cross_val_score(estimator = forest,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_rf_default_cv = rf_cv_default.mean()\nprint(\"Score (Radom Forest default CV): %f\" % score_rf_default_cv)\nscores_cv.append(score_rf_default_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.5-KNN\"></a>\n# 4.5 - KNN\n\n<a id=\"4.5.1-Build-A-Model-With-Default-Parameters\"></a>\n# 4.5.1 - KNN - Build a model with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = neighbors.KNeighborsClassifier(n_neighbors = 4)\nknn.fit(x_train, y_train)\nlabels_knn = knn.predict(x_test)\nscore_knn = knn.score(x_test, y_test)\nprint(\"Score (KNN): %f\" % score_knn)\nconf_mx_knn = confusion_matrix(y_test, labels_knn)\nscores.append(score_knn)\n#print(\"R2 Score %f \" % r2_score(y_test, labels_knn))\n#knn.score(x_test, y_test) , np.mean(labels_knn == y_test), (labels_knn == y_test).sum() / len(x_test), \"R2 Score %f \" % r2_score(y_test, labels_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.5.2-Parameters-Tuning\"></a>\n# 4.5.2 - KNN - Parameters tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_knn = {'n_neighbors': [5,7,9,11,13,15,17,19,21],\n              'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'weights' : ['uniform', 'distance']}\ngrid_search_knn = GridSearchCV(estimator = knn,\n                           param_grid = params_knn,\n                           scoring = 'accuracy',\n                           cv = 5)\ngrid_search_knn = grid_search_knn.fit(x, y)\nprint(f'The best parameters for KNN are: \"{grid_search_knn.best_params_}\" and this model can explain the dataset with an accuracy of {str(np.round(grid_search_knn.best_score_ * 100,2))} %')\nscores.append(grid_search_knn.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.5.3-Confusion-Matrix\"></a>\n# 4.5.3 - KNN - Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"DisplayConfusionMatrix(knn,\"Confusion matrix for KNN model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.5.4-Importance-Of-Each-Feature\"></a>\n# 4.5.4 - KNN - Importance of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To be implemented","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.5.5-Cross-Validation\"></a>\n# 4.5.5 - KNN - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation for KNN with default hyper parameters\nknn_cv_default = cross_val_score(estimator = knn,\n                             X = x, y = y,\n                             cv = 10, scoring = 'accuracy')\nscore_knn_default_cv = knn_cv_default.mean()\nprint(\"Score (KNN default CV): %f\" % score_knn_default_cv)\nscores_cv.append(score_knn_default_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5-Deep-Learning-TensorFlow-And-Keras\"></a>\n# 5 - Deep learning - Tensorflow and Keras\n\n\"Deep Learning com Python de A a Z - O Curso Completo\" - https://www.udemy.com/course/deep-learning-com-python-az-curso-completo/\nUdemy course from https://iaexpert.academy/\n\n\n\nTBD - Parameteres tuning"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5.1-Using-Test-Split\"></a>\n# 5.1 - Using test_split"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_split = Sequential()\n#classifier_split.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform', input_dim = 29))\nclassifier_split.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal', input_dim = 29))\n#classifier_split.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform'))\nclassifier_split.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal'))\nclassifier_split.add(Dense(units = 1, activation = 'sigmoid'))\n\notimizador = keras.optimizers.Adam(lr = 0.001, decay = 0.0001, clipvalue = 0.5)\nclassifier_split.compile(optimizer = otimizador, loss = 'binary_crossentropy',\n                      metrics = ['binary_accuracy'])\n\n# Fit model\nclassifier_split.fit(x_train, y_train,\n                  batch_size = 10, epochs = 100, verbose = 0)\n# Predict\nlabels_rn_split = classifier_split.predict(x_test)\nlabels_rn_split = (labels_rn_split > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DisplayConfusionMatrix_2(labels_rn_split, 'Confusion Matrix for Neural Network - Using split')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = accuracy_score(y_test, labels_rn_split)\nprint(precision)\nscores.append(precision)\nresultado = classifier_split.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5.2-Using-Cross-Validation\"></a>\n# 5.2 - Using cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"previsores = x\nclasse = y\n\ndef createNeuralNetwork():\n    classifier_cv = Sequential()\n    classifier_cv.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal', input_dim = 29))\n    #classifier_cv.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform', input_dim = 29))\n    classifier_cv.add(Dropout(0.2))\n    classifier_cv.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'normal'))\n    #classifier_cv.add(Dense(units = 16, activation = 'relu', kernel_initializer = 'random_uniform'))\n    classifier_cv.add(Dropout(0.2))\n    classifier_cv.add(Dense(units = 1, activation = 'sigmoid'))\n    otimizador = keras.optimizers.Adam(lr = 0.001, decay = 0.0001, clipvalue = 0.5)\n    classifier_cv.compile(optimizer = otimizador, loss = 'binary_crossentropy',\n                      metrics = ['binary_accuracy'])\n    return classifier_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_cv = KerasClassifier(build_fn = createNeuralNetwork,\n                                epochs = 100,\n                                batch_size = 10, verbose = 0)\nlabels_rn_cv = cross_val_score(estimator = classifier_cv,\n                             X = previsores, y = classe,\n                             cv = 10, scoring = 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = labels_rn_cv.mean()\nscores.append(mean)\nscores_cv.append(mean)\nstddev = labels_rn_cv.std()\nprint(mean)\nprint(stddev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6-Conclusion\"></a>\n# 6 - Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = ['SVM', 'SVM tunned','Decision tree','Decision tree tunned','Logistic regression','Logistic regression tunned', 'Random forest','Random forest tunned','KNN','KNN tunned', 'Neural network using split', 'Neural network using cross validation']\ndf_scores = pd.DataFrame({'Model': models,\n                       'Score': scores}).sort_values(['Score', 'Model'],ascending = [False, True])\ndf_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#models_cv = ['SVM CV', 'SVM tunned CV','Decision tree CV','Decision tree tunned CV','Logistic regression CV','Logistic regression tunned CV', 'Random forest CV','Random forest tunned CV','KNN CV','KNN tunned CV', 'Neural network using CV']\nmodels_cv = ['SVM CV','Decision tree CV','Logistic regression CV','Random forest CV','KNN CV','Neural network using CV']\ndf_scores_cv = pd.DataFrame({'Model CV': models_cv,\n                       'Score CV': scores_cv}).sort_values(['Score CV', 'Model CV'],ascending = [False, True])\ndf_scores_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = df_scores.iloc[0]\n#type(best_model.Score)\nprint(f'We can conclude that \"{best_model.Model}\" model can explain this dataset with an accuracy of {str(np.round(best_model.Score * 100,2))} %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7-Appendix\"></a>\n# 7 - Appendix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphical representation for Decision tree\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nexport_graphviz(tree, out_file=\"mytree.dot\")\nwith open(\"mytree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}